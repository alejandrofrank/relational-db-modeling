# Data modeling with Postgres

In this project we will create a Postgres database with tables designed to optimize queries on song play analysis for a company called Sparkify, a startup that have been collecting data in the form of json files but have no way to analyze it properly.

# Data information
All the data in this project is split up into two parts, the song dataset and the log dataset.
### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
For example, here are filepaths to two files in this dataset.
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
```py
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```
### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.
The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```
To approach this challenge we will be using pandas dataframes to convert the json data.

# Project parts
  - Create a database
  - Set up an ETL pipeline
  - Write some tests to see if the two steps above ran correctly

### Database implementation
For this part we will be writing a python file that creates a table schema, to be exact, a star schema.
The file `create_tables.py` will execute a set of PostgreSQL queries to drop existing tables and creating the whole schema. The queries will be located at the same level in the `sql_queries.py` file.
### Extract, transform and load
The extraction part of the ETL will be addressed in the `etl.py` script. The script will extract data from the json files and save it in a dataframe.
The transformation will occur inside of the `etl.py` aswell and for this part we will be using pandas to filter out rows inside of the json file previously extracted. 
The loading will take place in the same script `etl.py` but with the help of some previously written queries extracted from `sql_queries.py` script that will insert data into the corresponding tables and also maintain a quality control of the inserted data.

### Tests
As said in the [Rubrik](https://review.udacity.com/#!/rubrics/2500/view), the data must pass quality tests. All tests are written in `test.ipynb` notebook file and it makes sure that data was inserted into each of the tables and the amount of rows match the target rows.

# Final words
I really enjoyed this project because now I have a base understanding of how to make my own database using postgres or any relational database system. I will definately keep on trying to learn more complex ways of moving the data like only getting last day or only new records data to speed up proccesses. 
I will put my socials below, feel free to message me!
| Platform |
| ------ | 
| [Github](https://github.com/alejandrofrank) | 
| [LinkedIn](https://www.linkedin.com/in/alejandro-frank-5a2632179/) | 
